\documentclass{report}
\usepackage{geometry}
 \usepackage[frenchb]{babel}
 \usepackage{caption}
\usepackage{subcaption} 
\geometry{hmargin = 2cm,vmargin = 2cm}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[table]{xcolor}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{stmaryrd}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{hyperref} 
\usepackage{url}
\usepackage{pgfplotstable}
\usepackage{listings}
\usepackage{tikz}
\usetikzlibrary{arrows,positioning, calc}
\tikzstyle{vertex}=[draw,fill=white!15,circle,minimum size=20pt,inner sep=0pt]
\usepackage{color}
\definecolor{darkWhite}{rgb}{0.94,0.94,0.94}
\lstset{
  aboveskip=3mm,
  belowskip=-2mm,
  backgroundcolor=\color{darkWhite},
  basicstyle=\footnotesize,
  breakatwhitespace=false,
  breaklines=true,
  captionpos=b,
  commentstyle=\color{red},
  deletekeywords={...},
  escapeinside={\%*}{*)},
  extendedchars=true,
  framexleftmargin=16pt,
  framextopmargin=3pt,
  framexbottommargin=6pt,
  frame=tb,
  keepspaces=true,
  keywordstyle=\color{blue},
  language=python,
  literate=
  {²}{{\textsuperscript{2}}}1
  {⁴}{{\textsuperscript{4}}}1
  {⁶}{{\textsuperscript{6}}}1
  {⁸}{{\textsuperscript{8}}}1
  {€}{{\euro{}}}1
  {é}{{\'e}}1
  {è}{{\`{e}}}1
  {ê}{{\^{e}}}1
  {ë}{{\¨{e}}}1
  {É}{{\'{E}}}1
  {Ê}{{\^{E}}}1
  {û}{{\^{u}}}1
  {ù}{{\`{u}}}1
  {â}{{\^{a}}}1
  {à}{{\`{a}}}1
  {á}{{\'{a}}}1
  {ã}{{\~{a}}}1
  {Á}{{\'{A}}}1
  {Â}{{\^{A}}}1
  {Ã}{{\~{A}}}1
  {ç}{{\c{c}}}1
  {Ç}{{\c{C}}}1
  {õ}{{\~{o}}}1
  {ó}{{\'{o}}}1
  {ô}{{\^{o}}}1
  {Õ}{{\~{O}}}1
  {Ó}{{\'{O}}}1
  {Ô}{{\^{O}}}1
  {î}{{\^{i}}}1
  {Î}{{\^{I}}}1
  {í}{{\'{i}}}1
  {Í}{{\~{Í}}}1,
  morekeywords={*,...},
  numbers=left,
  numbersep=10pt,
  numberstyle=\tiny\color{black},
  rulecolor=\color{black},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  stepnumber=1,
  stringstyle=\color{gray},
  tabsize=4,
  title=\lstname,
}
%pour la compilation minted : C:\Users\Serge\Anaconda3\pkgs\pygments-2.2.0-py37_0\Scripts à ajouter en PATH et 
%remplacer pdflatex -synctex=1 -interaction=nonstopmode %.tex
%par pdflatex -synctex=1 -interaction=nonstopmode --shell-escape %.tex

%pour la gestion des théorème : 
\newtheorem*{theo}{\textbf{Théorème}}
\newtheorem{definition}{Définition}
\newtheorem*{petiteprop}{Proposition}
\newtheorem*{cor}{Corollaire}
\def\proofname{Preuve}

\title{Quadtree 3M101}
\author{Thiziri Baiche \and Charlotte Briquet \and Serge Durand \and Kevin Meetooa}
\graphicspath{ {./images/} }
\begin{document}
\maketitle
\tableofcontents
\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}
Les arbres sont une structure de donnée classique, aux nombreuses variations et applications. Notre objectif pour ce projet est d'étudier et d'implémenter des quadtree \footnote{nous utiliserons le terme quadtree dans le reste du rapport, préféré au terme "arbre quaternaire", pas très joli...}, avec plusieurs applications concrètes. Pour bien comprendre les quadtree nous devons d'abord étudier les arbres binaires. Après avoir introduit les notions et le vocabulaire de base sur les arbres binaires nous étudierons en particulier les arbres binaires de recherche avec une première implémentation. En effet les quadtree peuvent être vu comme une extension des arbres binaires de recherche. Nous présentons également des mesures expérimentales de la complexité des méthodes implémentées.

\chapter{Les Arbres Binaires}
\section{Description et vocabulaire des arbres binaires}
\subsection{Définitions générales sur les arbres}

Un \textit{arbre} est une structure de donnée hiérarchique définie par un nombre fini de nœuds. Chaque \textit{nœud} est composé d'une \textit{clef} (appelée également \textit{étiquette}) qui représente sa valeur ou l'information associée, et d'un ensemble de références vers d'autres noeuds. On dirat que l'arbre est \textit{étiqueté} sur l'ensemble E si ses étiquettes appartiennent à l'ensemble E. On dit que chaque nœud est relié par une \textit{branche}. 
On nomme des nœuds \textit{parents}, \textit{enfants} ou \textit{fils}, \textit{frères}, \textit{ancêtres} ou \textit{descendants}, les nœuds d'un arbre de manière analogue à un arbre généalogique. Dans un arbre, binaire ou non, un noeud a exactement un parent, sauf la \textit{racine} qui n'en a aucun : c'est la particularité de cette structure et ce qui donne le nom d'arbre. 

On dit qu'un noeud qui n'a aucun fils est une \textit{feuille}.
Le \textit{degré} d'un nœud est défini par le nombre de fils qu'il possède. Le degré maximal correspond au degré de l'arbre.
Un arbre \textbf{binaire} est donc un arbre de degré deux, c'est à dire que chaque noeud a au plus deux fils.
La \textit{taille} d'un arbre est son nombre total de nœuds.
Le \textit{chemin} d'un nœud est une suite de nœuds qu'il faut emprunter pour parcourir l'arbre de la racine au nœud en question. On appelle la \textit{longueur} d'un chemin, le nombre de nœuds empruntés.

La \textit{hauteur} (ou profondeur) d'un arbre est la longueur du chemin le plus long.
On considère la racine de l'arbre comme de niveau 1 puis à chaque génération le niveau augmente de 1.
On définit un \textit{sous-arbre} comme un autre arbre formé par un sous-ensemble de nœuds et de branches d'un arbre principal. En effet on peut considérer le fils d'un noeud comme la racine d'un nouvel arbre, un sous-arbre donc. 
Voilà quelques exemples d'arbres :

\subsection{Les arbres ordonnés}

On dit qu'un arbre étiqueté sur un ensemble muni d'une relation d'ordre totale est \textit{ordonné} si tous ses nœuds ont une étiquette supérieure ou égale à celle de chacun de ses enfants s'ils existent. Ainsi, l'étiquette de la racine a la valeur maximale. Pour tout chemin de l'arbre, les étiquettes se succèdent dans un ordre décroissant.

On appelle \textit{tas} ou \textit{arbre tassé} un arbre binaire ordonné presque complet : tous les niveaux de l'arbre binaire sont remplis, sauf peut-être le dernier qui est éventuellement rempli sur la gauche. On parle aussi d'arbre \textit{parfait}. La structure de tas est notamment utilisée pour le tri par tas. 

\subsection{Les arbres d'expression arithmétiques}

Un \textit{arbre binaire d'expression} est un genre d'arbre binaire utilisé pour représenter, comme son nom l'indique, des expressions. Il existe deux types d'expression qu'un arbre binaire peut représenter : algébrique et booléenne.
Les feuilles d'un arbre binaire d'expression sont des quantités (constantes ou variables) numérique dans le cas algébrique et "vrai" ($T$) ou "faux" ($F$) dans le cas booléen.
Les nœuds internes sont des opérateurs : addition ($+$), soustraction ($-$), multiplication ($\times$), division($\div$) et puissance ($\ldots^{\ldots}$) dans le cas algébrique ou des quantificateurs : "et" ($\wedge$), "ou" ($\vee$) et la négation ($\neg$) dans le cas booléen.
Un arbre d'expression est alors évalué en appliquant l'opérateur de la racine au valeurs obtenues en évaluant récursivement les sous-arbres de gauche et de droite, via un parcours \hyperref[suffixe]{suffixe}. 

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1,yscale=1]
\tikzstyle{fleche}=[->,>=latex,thick]
\tikzstyle{noeud}=[fill=white,circle]
\tikzstyle{feuille}=[fill=white,circle]
\def\DistanceInterNiveaux{1}
\def\DistanceInterFeuilles{1}
\def\NiveauA{(-0)*\DistanceInterNiveaux}
\def\NiveauB{(-1)*\DistanceInterNiveaux}
\def\NiveauC{(-2)*\DistanceInterNiveaux}
\def\NiveauD{(-3)*\DistanceInterNiveaux}
\def\InterFeuilles{(1)*\DistanceInterFeuilles}
\node[noeud] (R) at ({(2.5)*\InterFeuilles},{\NiveauA}) {$\times$};
\node[noeud] (Ra) at ({(1.5)*\InterFeuilles},{\NiveauB}) {$\div$};
\node[noeud] (Raa) at ({(0.5)*\InterFeuilles},{\NiveauC}) {$-$};
\node[feuille] (Raaa) at ({(0)*\InterFeuilles},{\NiveauD}) {};
\node[feuille] (Raab) at ({(1)*\InterFeuilles},{\NiveauD}) {$8$};
\node[noeud] (Rab) at ({(2.5)*\InterFeuilles},{\NiveauC}) {$+$};
\node[feuille] (Raba) at ({(2)*\InterFeuilles},{\NiveauD}) {$x$};
\node[feuille] (Rabb) at ({(3)*\InterFeuilles},{\NiveauD}) {$5$};
\node[noeud] (Rb) at ({(4.5)*\InterFeuilles},{\NiveauB}) {$\ldots^{\ldots}$};
\node[feuille] (Rba) at ({(4)*\InterFeuilles},{\NiveauC}) {$4$};
\node[feuille] (Rbb) at ({(5)*\InterFeuilles},{\NiveauC}) {$2$};
\draw[fleche] (R)--(Ra);
\draw[fleche] (Ra)--(Raa);
\draw[fleche] (Raa)--(Raab);
\draw[fleche] (Ra)--(Rab);
\draw[fleche] (Rab)--(Raba);
\draw[fleche] (Rab)--(Rabb);
\draw[fleche] (R)--(Rb);
\draw[fleche] (Rb)--(Rba);
\draw[fleche] (Rb)--(Rbb);
\end{tikzpicture}
\caption{Exemple d'arbre binaire d'expression algébrique} \label{fig:Exemple d'arbre binaire d'expression algébrique}
\end{center}
\end{figure} 

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1,yscale=1]
\tikzstyle{fleche}=[->,>=latex,thick]
\tikzstyle{noeud}=[fill=white,circle]
\tikzstyle{feuille}=[fill=white,circle]
\def\DistanceInterNiveaux{1}
\def\DistanceInterFeuilles{1}
\def\NiveauA{(-0)*\DistanceInterNiveaux}
\def\NiveauB{(-1)*\DistanceInterNiveaux}
\def\NiveauC{(-2)*\DistanceInterNiveaux}
\def\NiveauD{(-3)*\DistanceInterNiveaux}
\def\InterFeuilles{(1)*\DistanceInterFeuilles}
\node[noeud] (R) at ({(2.5)*\InterFeuilles},{\NiveauA}) {$\vee$};
\node[noeud] (Ra) at ({(1.5)*\InterFeuilles},{\NiveauB}) {$\wedge$};
\node[noeud] (Raa) at ({(0.5)*\InterFeuilles},{\NiveauC}) {$\neg$};
\node[feuille] (Raaa) at ({(0)*\InterFeuilles},{\NiveauD}) {$F$};
\node[feuille] (Raab) at ({(1)*\InterFeuilles},{\NiveauD}) {};
\node[noeud] (Rab) at ({(2.5)*\InterFeuilles},{\NiveauC}) {$\vee$};
\node[feuille] (Raba) at ({(2)*\InterFeuilles},{\NiveauD}) {$T$};
\node[feuille] (Rabb) at ({(3)*\InterFeuilles},{\NiveauD}) {$F$};
\node[noeud] (Rb) at ({(4.5)*\InterFeuilles},{\NiveauB}) {$\vee$};
\node[feuille] (Rba) at ({(4)*\InterFeuilles},{\NiveauC}) {$T$};
\node[feuille] (Rbb) at ({(5)*\InterFeuilles},{\NiveauC}) {$F$};
\draw[fleche] (R)--(Ra);
\draw[fleche] (Ra)--(Raa);
\draw[fleche] (Raa)--(Raaa);
\draw[fleche] (Ra)--(Rab);
\draw[fleche] (Rab)--(Raba);
\draw[fleche] (Rab)--(Rabb);
\draw[fleche] (R)--(Rb);
\draw[fleche] (Rb)--(Rba);
\draw[fleche] (Rb)--(Rbb);
\end{tikzpicture}
\caption{Exemple d'arbre binaire d'expression booléen} \label{fig:Exemple d'arbre binaire d'expression booléen}
\end{center} 
\end{figure}


\section{Particularité des arbres binaires}

On donne la définition inductive de l'ensemble des arbres binaires étiqueté sur un ensemble E, qu'on note AB:
\begin{flushleft}
    \bf
    \underline{Base :}
    
\end{flushleft}
$\emptyset \in AB$
\begin{flushleft}
    \bf
    \underline{Induction :}
\end{flushleft}
    $\forall x \in E, G \in AB, D \in AB : (x,G,D) \in AB$
    
Un arbre binaire est ainsi un triplet constitué de :
\begin{itemize}
\item un nœud racine
\item un sous-arbre gauche
\item un sous-arbre droit
\end{itemize}
éventuellement vides.

Un arbre binaire qui ne contient pas de nœuds est appelé un arbre vide ou un arbre nul.
Si le sous-arbre de gauche n'est pas vide, sa racine est appelée le fils gauche de la racine de l'arbre entier. Il en va de même pour le sous-arbre de droite.
Par exemple, dans l'arbre (a), le nœud « 2 » est le fils de gauche de la racine de l'arbre, il est aussi la racine du sous-arbre de gauche.
Si un sous-arbre est un arbre nul, on dit alors que le fils est absent ou manquant.
\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[xscale=1,yscale=1]
\tikzstyle{fleche}=[->,>=latex,thick]
\tikzstyle{noeud}=[fill=white,circle]
\tikzstyle{feuille}=[fill=white,circle]
\def\DistanceInterNiveaux{1}
\def\DistanceInterFeuilles{1}
\def\NiveauA{(-0)*\DistanceInterNiveaux}
\def\NiveauB{(-1)*\DistanceInterNiveaux}
\def\NiveauC{(-2)*\DistanceInterNiveaux}
\def\NiveauD{(-3)*\DistanceInterNiveaux}
\def\InterFeuilles{(1)*\DistanceInterFeuilles}
\node[noeud] (R) at ({(2)*\InterFeuilles},{\NiveauA}) {$3$};
\node[noeud] (Ra) at ({(1)*\InterFeuilles},{\NiveauB}) {$2$};
\node[noeud] (Raa) at ({(0.5)*\InterFeuilles},{\NiveauC}) {$1$};
\node[feuille] (Raaa) at ({(0)*\InterFeuilles},{\NiveauD}) {$6$};
\node[feuille] (Raab) at ({(1)*\InterFeuilles},{\NiveauD}) {};
\node[feuille] (Rab) at ({(2)*\InterFeuilles},{\NiveauC}) {$4$};
\node[noeud] (Rb) at ({(3.5)*\InterFeuilles},{\NiveauB}) {$7$};
\node[feuille] (Rba) at ({(3)*\InterFeuilles},{\NiveauC}) {$5$};
\node[feuille] (Rbb) at ({(4)*\InterFeuilles},{\NiveauC}) {};
\draw[fleche] (R)--(Ra);
\draw[fleche] (Ra)--(Raa);
\draw[fleche] (Raa)--(Raaa);
\draw[fleche] (Ra)--(Rab);
\draw[fleche] (R)--(Rb);
\draw[fleche] (Rb)--(Rba);
\end{tikzpicture}
\end{center}
\caption{Arbre (a)} \label{fig:Exemples d'arbres}
\end{figure}

\subsection{Premiers algorithmes}
\subsubsection{Notations de Landau}
Pour la description des algorithmes et l'analyse de leur complexité on utilisera les notations usuelles (de Landau). On les rappelle ici :
$f$ et $g$ désignent des fonctions de $\mathbb{N}$ dans $\mathbb{N}$.
\begin{itemize}
\item $f \in \mathcal{O}(g)$ si $\exists D > 0$ et $n_0 \geq 0$ tels que $\forall n \geq n_0, f(n) \leq Dg(n)$
\item $f \in \Omega(g)$ si $\exists C >0$ et $n_0 \geq 0$ tels que $\forall n \geq n_0, Cg(n) \leq f(n)$.
\item $f \in \Theta(g)$ si $\exists C>0, D>0$ et $n_0 \geq 0$ tels que $\forall n \geq n_0, Cg(n) \leq f(n) \leq Dg(n)$.
\end{itemize}

Autrement dit : $f \in \mathcal{O}(g)$ si f est inférieure à g à partir d'un certain rang, à une constante multiplicative près. 
De plus on remarque que $f \in \mathcal{O}(g) \iff g \in \Omega(f)$ et $f \in \Theta(g) \iff (f \in \mathcal{O}(g)$ et $f \in \Omega(g))$.
\subsubsection{calcul de la hauteur et de la taille}
\paragraph{}
Donnons d'abord des définitions inductives de la hauteur et de la taille d'un arbre. Nous notons $h(T)$ la hauteur de l'arbre T, et $n(T)$ sa taille. On a :
\begin{flushleft}
    \bf
    \underline{Base :}
    
\end{flushleft}
$h(\emptyset) = 0$ \\
$n(\emptyset) = 0$ \\
\begin{flushleft}
    \bf
    \underline{Induction :}
\end{flushleft}
    $\forall x \in E, G \in AB, D \in AB$ : \\
    $h((x,G,D))= max(h(G),h(D))+1$ \\
    $n((x,G,D))= n(G)+n(D)+1$
    
\paragraph{}
Suivant cette définition, pour calculer la hauteur d'un arbre on peut utiliser la fonction récursive suivante :
\begin{lstlisting}
def hauteur(self):
		"""convention : arbre vide de hauteur 0
		arbre réduit à un noeud : hauteur 1"""
		if self.clef == None:
			return 0
		if self.gauche is None and self.droit is None:
			return 1
		return max(self.gauche.hauteur(),self.droit.hauteur())+1
\end{lstlisting}
\paragraph{Complexité, terminaison et correction :} On fait une preuve détaillée pour la terminaison, la correction et la complexité de cette méthode.
Pour le calcul de la complexité on compte le nombre d'addition. On pose $c(n) = $ le nombre d'addition lors de l'appel $hauteur(T)$ sur un arbre $T$ de taille $n$. On montre par induction sur l'ensemble $AB$ que $c(n) = n$ pour tout $T \in AB$ tel que $n(T) = n$, et aussi que la fonction se termine et est correcte.

\begin{flushleft}
    \bf
    \underline{Base :}
    
\end{flushleft}
$h(\emptyset) = 0$ \\ 
L'appel retourne $0$ directement, il n'y a aucune addition : $c(0) = 0$. La fonction se termine et est correcte.
\begin{flushleft}
    \bf
    \underline{Induction :}
\end{flushleft}
On fait une récurrence forte : supposons que $\forall k \in \llbracket 0, n-1 \rrbracket, c(k) = k$, c'est à dire que l'appel $hauteur(A)$ comporte $k$ additions pour tout arbre $A$ de hauteur $k$ et qu'il se termine et retourne $h(A)$

Soit $T = (x,T_1,T_2)$ tel que $n(T) = n$. On pose $n_1 = n(T_1)$ et $n_2=n(T_2)$. On a $n_1 + n_2 + 1 = n$ donc $(n_1,n_2) \in {\llbracket 0, n-1 \rrbracket}^2$. 

Puisque T n'est pas vide on distingue deux cas pour l'appel $hauteur(T)$ provoque deux appels récursifs : $hauteur(T_1)$ et $hauteur(T_2)$ et renvoie 1 + le max des valeurs retournées par ces appels. Or par hypothèse de récurrence les appels $hauteur(T_1)$ et $hauteur(T_2)$ se terminent, renvoient respectivement $h(T_1)$ et $h(T_2)$ et comportent respectivement $n_1$ et $n_2$ additions. Donc l'appel hauteur(T) se termine, renvoie $max(h(T_1),h(T_2))+1 = h(T)$ et comporte $n_1 + n_2 + 1 = n$ additions.
\begin{flushleft}
    \bf
    \underline{Conclusion :}
\end{flushleft}

On a montré que la propriété est vraie pour $T = \emptyset$, et que si elle est vraie pour tout $A \in AB$ tel que $n(A) < n$ alors elle est vraie pour tout $T \in AB$ tel que $n(T) = n$. En conclusion la propriété est vraie pour tout arbre binaire.

En pratique on a dû distinguer le cas où T est réduit à un noeud, les appels T.hauteur() ne fonctionnant pas en Python si T est None, mais cela n'a pas d'impact sur la complexité.

La méthode pour calculer la taille d'un arbre peut être implémentée de manière récursive également, avec un résultat et une preuve analogue sur la complexité, le nombre d'additions étant alors 2n pour un arbre de taille n.

\section{Relations entre hauteur et taille}

On rappelle qu'un arbre complet est un arbre dont tout les nœuds internes ont deux fils. 
C'est un arbre entièrement rempli.
Un arbre parfait est un arbre dont tout les niveaux sont remplis sauf éventuellement le dernier, en général rempli le plus à gauche. 
On introduit également la notion d' \textit{arbre H-équilibré} : arbre dont la différence de profondeur entre deux feuilles est au plus 1. Tout les niveaux sont donc rempli sauf le dernier ici aussi, sans condition sur la répartition des feuilles du dernier niveau. 

Illustration : 

\begin{figure}[h]
\centering
\begin{subfigure}[b]{.3\textwidth}
\begin{tikzpicture}[very thick,level/.style={sibling distance=30mm/#1},scale=0.8]
\node [vertex] {$15$}
   child {
    node [vertex]  {$7$}
    child{node [vertex] {$6$}}
    child{node [vertex] {$10$}}
   }
   child {
    node [vertex] {$17$}
    child{node [vertex] {$16$}}	
    child{node [vertex] {$20$}}
   };
\end{tikzpicture}
\caption{arbre complet}
\end{subfigure}
\begin{subfigure}[b]{.3\textwidth}
\begin{tikzpicture}[very thick,level/.style={sibling distance=30mm/#1},scale=0.8]
\node [vertex] {$15$}
   child {
    node [vertex]  {$7$}
    child{node [vertex] {$6$}}
    child{ edge from parent[draw=none]}
   }
   child {
    node [vertex] {$17$}
    child{ edge from parent[draw=none]}	
    child{ edge from parent[draw=none]}
   };
\end{tikzpicture}
\caption{arbre parfait}
\end{subfigure}
\begin{subfigure}[b]{.3\textwidth}
\begin{tikzpicture}[very thick,level/.style={sibling distance=30mm/#1},scale=0.8]
\node [vertex] {$10$}
   child {
    node [vertex]  {$7$}
    child{node [vertex] {$6$}}
    child{ edge from parent[draw=none]}
   }
   child {
    node [vertex] {$17$}
    child{node [vertex] {$16$}}	
    child{node [vertex] {$20$}}
   };
\end{tikzpicture}
\caption{arbre H-équilibré}
\end{subfigure}
\end{figure}

On remarque que tout arbre binaire $(x,G,D)$ est complet si et seulement si $G$ et $D$ sont complets et de même hauteur.

\paragraph{}
On a les propriétés suivantes pour les arbres complets :
\begin{petiteprop}[relation entre taille et hauteur]
La hauteur d'un arbre complet ($h$) et sa taille ($n$) vérifie la relation suivante :
\begin{align}
2^h-1 &= n \\
\llap{$\Leftrightarrow$ \qquad \qquad} h &= log(n+1)
\end{align}
\end{petiteprop}

\begin{proof}
La preuve est immédiate par récurrence sur la hauteur :
\paragraph{\underline{Base :}}
$h = 0 \Leftrightarrow$ l'arbre est vide. Or $2^0-1 = 1-1 = 0$ et il y a bien 0 nœud dans l'arbre vide.
\paragraph{\underline{Induction :}}
Supposons que la propriété soit vérifiée pour tout arbre binaire complet de hauteur $h-1$.
Soit $(x,G,D)$ un arbre binaire complet de hauteur $h$ et de taille $n$. 
Alors $G$ et $D$ sont des arbres binaires complets de hauteur $h-1$. Alors $n = n_G + n_D + 1 = 2^{h-1}-1 + 2^{h-1}-1 + 1 = 2*(2^{h-1}) -1 = 2^h-1$.
La propriété est vraie pour un arbre vide et se propage, elle est donc vérifiée pour tout arbre complet. Il suffit ensuite de prendre le logarithme de chaque côté de (1) pour avoir la deuxième égalité. 
\end{proof}

\paragraph{Conséquences :} On a $log(n) \leq log(n+1) \leq log(n^2) = 2log(n)$ donc $log(n+1) = \Theta(log(n))$. \textbf{Les algorithmes de complexité linéaire en la hauteur d'un arbre sont donc logarithmique en la taille de l'arbre pour les arbres complets}.

De plus si on a une certaine garantie sur la structure d'un arbre, notamment sur son équilibre, on a alors des bornes sur la complexité des fonctions.
En effet si $T$ est un arbre H-équilibré de hauteur $h$, sa taille, $n$, est comprise entre la taille d'un arbre complet de hauteur $h-1$ et celle d'un arbre complet de hauteur $h$. 
Ce qui donne $h-1 \leq log(n-1)$ et $log(n-1) \leq h$.
Les algorithmes linéaire en la hauteur sont donc également logarithmique en la taille pour les arbres H-équilibré. Pour garantir une certaine efficacité des algorithmes l'enjeu est donc de construire des arbres le plus équilibré possible.

\begin{petiteprop}
Un ABR complet non vide a $2^{h-1}$ feuilles.
\end{petiteprop}
La preuve est immédiate par récurrence sur la hauteur. On ne la détaille pas, le procédé est le même que précédemment, il suffit de remarquer que pour tout arbre complet non vide $T = (x,G,D)$, $f(T) = f(G) + f(D) = 2*f(G)$, avec $f(T) =$ le nombre de feuilles de T.
On en déduit un corollaire assez intéressant: 

\begin{cor}
Plus de la moitié des noeuds dans un ABR complet sont des feuilles. 
\end{cor}
\begin{proof}
On calcule le rapport entre le nombre de feuilles et le nombre de noeuds. 
D'après la proposition précédente, ce rapport vaut $\dfrac{2^{h-1}}{2^{h}-1 } \geq \dfrac{2^{h-1}}{2^{h}} = \dfrac{1}{2}$ 
\end{proof}

\begin{theo}[hauteur moyenne d'un noeud]
La profondeur moyenne d'un noeud choisi au hasard est en $\Theta(log(n))$
\end{theo}

\begin{proof}
Par définition d'un ABR complet non vide, il y a $2^{h-1}$ noeuds à l'étage h de l'arbre, cela est dû au fait que chaque noeud qui n'est pas une feuille possède exactement 2 fils. 

Ainsi, il y a 1 noeud à l'étage 1 (la racine), 2 noeuds à l'étage 2 (les 2 fils de la racine), 4 noeuds à l'étage 3 (les 2 fils des 2 noeuds de l'étage 1) et ainsi de suite... 

Pour calculer la profondeur moyenne d'un noeud, il suffit de calculer le rapport entre la somme des profondeurs de chaque noeud de l'arbre et le nombre de noeuds total. 

Ce rapport vaut :
$\dfrac{\sum_{k=0}^{h-1} k*2^k}{2^{h}-1}$ = $\dfrac{2^{h}*h-2*(2^{h}-1)}{2^{h}-1}$ = $\dfrac{(h-1)*2^{h}-2^{h}+2}{2^{h}-1}$ (la valeur de la somme a été calculée à l'ordinateur) \\
Et $\dfrac{(h-1)*(2^{h}-1)+(h-1)-(2^{h}-1)+1}{2^{h}-1}$ = $h-1 + \dfrac{h}{2^{h}-1}$ avec $\lim_{h\to\infty} \dfrac{h}{2^{h}-1}=0$ 

Donc la hauteur moyenne d'un noeud vaut $h-1=\Theta(log(n))$ (d'après un théorème précédent)
\end{proof}

\chapter{Les Arbres Binaires de recherche}
\section{Définition}
Un arbre binaire de recherche est un arbre binaire étiqueté sur un ensemble muni d'un ordre total (typiquement les réels ou les entiers) dans lequel tout noeud a une étiquette supérieure à celles de son sous arbre gauche et inférieure à celles de son sous arbre droit. L'abréviation ABR ou BST pour \textit{binary search tree} est fréquemment utilisée. On donne aussi la définition inductive suivante :
\begin{flushleft}
    \bf
    \underline{Base :}
    
\end{flushleft}
$\emptyset \in ABR$
\begin{flushleft}
    \bf
    \underline{Induction :}
\end{flushleft}
    $\forall x \in E, G \in ABR, D \in ABR$, si :
    \begin{itemize}
        \item $\forall (x_1,G_1,D_1) \in G, x_1 < x$
        \item $\forall (x_2,G_2,D_2) \in D, x_2 > x$
    \end{itemize}
    alors $(x,G,D) \in ABR$

\section{Parcours et affichage}

Pour réaliser les différents parcours décrits ci-dessous, on peut définir un arbre par un dictionnaire qui associe chaque nœud à ses fils. Par exemple, l'arbre (a) est définit par :
\begin{lstlisting}
A = {3:[2, 7], 2:[1, 4], 7:[5], 1:[6], 4:[], 5:[], 6:[]}
\end{lstlisting}
C'est cette méthode qui est utilisée pour décrire un arbre dans le cas du parcours en largeur. Cependant, on peut également définir une classe Arbre telle que :
\begin{lstlisting}
class Arbre: 
    def __init__(self,valeur): 
        self.gauche = None
        self.droit = None
        self.racine = valeur
# L'arbre (a) est donc défini par :
A = Arbre(3) 
A.gauche = Arbre(2) 
A.droit = Arbre(7) 
A.gauche.gauche = Arbre(1) 
A.gauche.droit = Arbre(4)
A.gauche.gauche.gauche = Arbre(6)
A.droit.gauche = Arbre(5)
\end{lstlisting}
On utilisera cette classe pour les parcours en profondeur.
\subsection{Parcours en largeur}
Le principe d'un parcours en largeur est de lister les nœuds de l'arbre niveau par niveau en commençant par les nœuds de niveau 1 puis les nœuds de niveau 2 et ainsi de suite. Dans chaque niveau, les nœuds sont parcourus de gauche à droite.
Le parcours en largeur de l'arbre (a) est donc $[3, 2, 7, 1, 4, 5, 6]$.
Contrairement aux parcours suivant le parcours en largeur s'implémente naturellement par un algorithme itératif.
L'algorithme d'un tel parcours se fait à l'aide d'une file (premier entré, premier sorti). On enfile d'abord la racine puis on utilise une boucle tant que la file n'est pas vide.
On défile la racine que l'on traite (typiquement par un affichage mais cela peut être un autre traitement), on enfile les fils de la racine en commençant par le fils gauche et on répète la boucle. Et ainsi de suite jusqu'à ce que la file soit vide.
\begin{lstlisting}
def parcours_largeur(arbre, racine):
	liste = [racine]
	parcours = [racine]
	while liste :
		x = liste.pop(0)
		for fils in arbre[x]:
			if fils in liste : 
				continue
			parcours.append(fils)
			liste.append(fils)
	return parcours
\end{lstlisting}
Pour les arbres la complexité d'un tel parcours est en $\Theta(n)$ : on fait exactement n tours de boucle. 
Remarquons que cet algorithme peut être utilisé sur n'importe quel type d'arbre, et même sur des graphes en général, avec quelques modifications pour ne pas repasser sur le même noeud et avoir une boucle infinie dans le cas de graphe cyclique par exemple.

\subsection{Parcours en profondeur}
Le principe d'un parcours en profondeur est de lister les nœuds de l'arbre récursivement à partir de la racine puis les sous-arbres gauches et droits de cette racine et ainsi de suite pour la totalité de l'arbre.
Il existe plusieurs types de parcours en profondeur : Infixe, Suffixe (ou Postfixe) et Préfixe.
\begin{itemize}
\item Infixe

Le parcours infixe consiste à lister les nœuds en partant du sous-arbre gauche puis remonter à sa racine et enfin parcourir le sous-arbre droit.
Le parcours infixe de l'arbre (a) est donc $[6, 1, 2, 4, 3, 5, 7]$.
\begin{lstlisting}
def parcours_infixe(arbre): 
    if arbre: 
        parcours_infixe(arbre.gauche) 
        print(arbre.racine)
        parcours_infixe(arbre.droit) 
\end{lstlisting}

\item Suffixe \label{suffixe}

Le parcours suffixe consiste quant à lui à lister les nœuds depuis le sous-arbre gauche puis le sous-arbre droit et enfin remonter à la racine.
Le parcours suffixe de l'arbre (a) est donc $[6, 1, 4, 2, 5, 7, 3]$.
\begin{lstlisting}
def parcours_suffixe(arbre): 
     if arbre: 
        parcours_suffixe(arbre.gauche) 
        parcours_suffixe(arbre.droit) 
        print(arbre.racine)
\end{lstlisting}

\item Préfixe

Le parcours préfixe, finalement, consiste à lister les nœuds en commençant par la racine puis le sous-arbre gauche et enfin le sous-arbre droit.
Le parcours préfixe de l'arbre (a) est donc $[3, 2, 1, 6, 4, 7, 5]$.
\begin{lstlisting}
def parcours_prefixe(arbre):
    if arbre:
        print(arbre.racine)
        parcours_prefixe(arbre.gauche)
        parcours_prefixe(arbre.droit)
\end{lstlisting}

\end{itemize}

Dans les trois cas la complexité est également en $\Theta(n)$, on passe exactement une fois sur chaque noeud. Ces parcours sont applicables aux arbres binaires en général, de recherche ou non, comme par exemple pour évaluer un arbre d'expression arithmétique.

\section{Méthodes principales spécifiques aux ABR}
\subsection{Recherche :} 
La recherche se fait de manière récursive, l'algorithme est assez simple il suffit de comparer la valeur qu'on recherche avec la clef de la racine et voir sur quel côté continuer la recherche, il y a 4 cas à traiter:
\begin{enumerate}
	\item la valeur qu'on recherche se trouve à la racine, dans ce cas on renvoie directement la racine.
	\item la valeur qu'on recherche est plus petite que la racine ce qui veut dire qu'elle est forcément dans le sous-arbre gauche donc on fait un appel récursif sur ce dernier.
	\item la valeur est plus grande que la racine on fait donc un appel récursif sur le sous-arbre droit.
	\item et finalement si on atteint une feuille et que sa clé n'est pas l'élément qu'on recherche ça veut dire que l'élément ne se trouve pas dans l'arbre.
\end{enumerate}

On peut alors utiliser la fonction récursive suivante:
\begin{lstlisting}
	def recherche(self,x):
		if self.clef == None:
			return False

		if x==clef:
			return self
		
		if x<self.clef:
			if self.gauche is not None:
				return self.gauche.recherche(x)
			else :
				return False
		elif x > self.clef:
			if self.droit is not None:
				return self.droit.recherche(x)
			else: 
				return False
\end{lstlisting}

\paragraph{Complexité, terminaison et correction :} On fait une preuve détaillée pour la terminaison, la correction et la complexité de cette fonction.

\begin{itemize}
    \item Complexité : le nombre d’opérations de la recherche dépend de la hauteur de l’arbre. Nous étudions ici le nombre de comparaisons.
    \begin{itemize}
	\item[-] Meilleur cas : l'élément cherché est à la racine. La fonction fait un retour immédiat, l'opération est en temps constant : 1 comparaison.
	
	\item[-] Pire cas : l'élément cherché est dans la feuille de plus grande profondeur (ie de profondeur h = la hauteur de l'arbre). On fait exactement h-1 appels récursifs après l'appel initial donc h comparaisons, jusqu'à arriver à la feuille
    \end{itemize}
    La recherche est donc en $\Omega(1)$ et $\mathcal{O}(h)$, c'est à dire linéaire en la hauteur dans le pire cas. Nous avons vu que $h = \Theta(log(n))$ pour les arbres complets ou équilibrés, dans ce cas la recherche est donc en $\mathcal{O}(log(n))$. Elle est en $\mathcal{O}(n)$ dans le cas d'arbres filiformes. Nous étudierons plus finement la complexité moyenne en fonction de la taille pour le rendu final du projet.


\item Terminaison et correction :

montrons que l'appel $T.recherche(x)$ se termine et est correct pour tout arbre de recherche $T$ étiqueté sur un ensemble $E$ et pour tout $x \in E$. On fait une preuve par induction sur la hauteur. 
    \begin{flushleft}
    \underline{Base :}
    \end{flushleft}
    Si h=0, alors l'arbre est vide et la fonction renvoie False quelque soit x, ce qui est le résultat attendu.
    
    \begin{flushleft}
    \underline{Induction :}
    \end{flushleft}
    
    Supposons que la fonction se termine et est correcte pour tout arbre de hauteur $p \in {\llbracket 0, h \rrbracket}$ et pour tout élément $x \in E$.
    
    Soit $T = (y,G,D)$ un ABR de hauteur $h+1$ et soit $x \in E$.
    
    Lors de l'appel $T.recherche(x)$ on distingue trois cas:
    \begin{enumerate}
    \item $y = x$ alors la fonction se termine et renvoie True, ce qui est le retour valide.
    \item $ x < y$ on a alors un appel récursif $G.recherche(x)$. Or on sait que x ne peut pas être dans D car T est un arbre binaire de recherche. De plus par hypothèse de récurrence l'appel $G.recherche(x)$ se termine et renvoie True si x est dans G, false sinon. Donc l'appel $T.recherche(x)$ se termine et est valide.
    \item $x > y$ ici on sait que x ne peut pas être dans G, puis que $D.recherche(x)$ se termine et est valide : l'appel $T.recherche(x)$ se termine et est valide également.
    \end{enumerate}
    Dans les trois cas l'appel $T.recherche(x)$ se termine et est valide.
\begin{flushleft}
    \underline{Conclusion :}
    \end{flushleft}

On a montré que l'appel $T.recherche(x)$ se termine et est valide pour l'arbre vide puis que s'il est valide et se termine pour tout arbre de hauteur $p \leq h$ alors il est valide et se termine pour tout arbre de hauteur $h+1$, donc l'appel $T.recherche(x)$ se termine et est valide pour tout arbre binaire de recherche.
\end{itemize}

Remarque : le cas de l'arbre vide peut se rencontrer de deux manières, un appel directement sur un arbre vide et à la fin de la récursion lorsqu'on arrive aux feuilles. Les deux cas sont distincts dans notre implémentation. En effet si T est de type None on ne peut pas faire l'appel $T.recherche(x)$, c'est pour cela que l'on fait une vérification avant de faire les appels récursifs. Cependant si $T$ a été initialisé comme un arbre contenant une seul clef vide ($T = Arbre(None)$), l'appel ne provoque pas de problème pour Python et il faut prévoir ce cas, c'est le but le la ligne 2. 

Pour les méthodes suivantes la complexité, la terminaison et la correction peuvent être prouvées de manière tout à fait similaire, nous ne détaillons pas ces preuves. 

\subsection{Insertion :}
Pour l'insertion on procède de manière analogue à la recherche : pour insérer un élément il faut savoir dans quelle partie de l'arbre le mettre pour que cela reste un arbre binaire de recherche.
Pour cela il suffit de comparer la valeur à insérer au noeud courant. On distingue trois cas :
\begin{enumerate}
    \item les valeurs sont égales : on fait un retour, il n'y a rien à insérer. On rappelle que dans un arbre binaire de recherche les étiquettes des noeuds sont distinctes.
    \item la valeur à insérer est plus petite que le noeud courant. On a alors deux possibilités :
    \begin{itemize}
        \item le sous arbre gauche est vide : on procède à l'insertion en créant un noeud avec pour étiquette la valeur à insérer, et des sous arbres gauche et droit vides.
        \item le sous arbre gauche n'est pas vide : on fait un appel récursif sur cet arbre.
    \end{itemize}
    \item si elle est plus grande on procède de manière analogue sur le sous arbre droit.
\end{enumerate}
On remarque que dans cette version on insère toujours au niveau des feuilles, si insertion il y a. L'algorithme est simple, mais il n'est pas optimal sur la structure de l'arbre, dans le sens où on ne cherche pas à garantir une structure équilibrée. Nous verrons plus tard (c'est prévu pour le rapport final) qu'il est possible de le faire en introduisant la rotation d'arbre.
La complexité est en $\mathcal{O}(h)$ avec $h$ la hauteur de l'arbre, comme pour la recherche. La preuve est analogue à la preuve de la méthode recherche.
\subsection{Suppression :} 
La fonction de suppression se fait de manière récursive. Elle est assez simple mais il faut juste faire attention à ne pas perdre des informations ou à modifier la structure d'arbre,ici aussi on 3 cas principaux :
notons d'abord x l'élément qu'on veut supprimer: 
\begin{enumerate}
\item si x c'est la racine de notre arbre, c'est un peu subtil, il faut aussi traiter trois cas:
\begin{itemize}
    \item[•] si x n'a pas de fils on le supprime directement.
    \item[•] si x a un seul fils on le remplace par celui-ci.
    \item[•] s'il en a deux on échange le x soit avec le max du sous arbre gauche (qui correspond au fils le plus a droite du sous arbre gauche), soit avec le min du sous arbre droit(qui correspond au fils le plus à gauche du sous arbre droit), puis on supprime le max ou le min d'après ce qu'on a choisit en faisant un appel récursif.
\end{itemize}
\item si x est plus petit que la racine on fait un appel récursif sur le sous arbre gauche 
\item si x est plus grand on fait un appel récursif sur le sous arbre droit.
\end{enumerate}
\begin{itemize}
    \item Complexité :
\end{itemize} 
Une suppression comporte donc un parcours d’une
unique branche, un éventuel échange de valeur, puis
la suppression d’un nœud possédant au maximum 1 successeur.
Les deux dernières opérations peuvent être
implémentées en $\Theta(1)$.
La suppression a donc, comme les autres opérations,
un coût maximal en $\Theta(log_2(n))$ si l’arbre est équilibré.

\begin{figure}[h]
\centering
\begin{subfigure}[b]{.3\textwidth}
\begin{tikzpicture}[very thick,level/.style={sibling distance=30mm/#1}]
\node [vertex,fill=red!15] {$15$}
   child {
    node [vertex]  {$7$}
    child{node [vertex] {$6$}}
    child{node [vertex] {$10$}}
   }
   child {
    node [vertex] {$17$}
    child{node [vertex] {$16$}}	
    child{node [vertex] {$20$}}
   };
\end{tikzpicture}
\end{subfigure}
\begin{subfigure}[b]{.3\textwidth}
\begin{tikzpicture}[very thick,level/.style={sibling distance=30mm/#1}]
\node [vertex,fill=red!15] {$15$}
   child {
    node [vertex]  {$7$}
    child{node [vertex] {$6$}}
    child{node [vertex,fill=green!15] {$10$}}
   }
   child {
    node [vertex] {$17$}
    child{node [vertex] {$16$}}	
    child{node [vertex] {$20$}}
   };
\end{tikzpicture}
\end{subfigure}
\begin{subfigure}[b]{.3\textwidth}
\begin{tikzpicture}[very thick,level/.style={sibling distance=30mm/#1}]
\node [vertex,fill=green!15] {$10$}
   child {
    node [vertex]  {$7$}
    child{node [vertex] {$6$}}
    child{ edge from parent[draw=none]}
   }
   child {
    node [vertex] {$17$}
    child{node [vertex] {$16$}}	
    child{node [vertex] {$20$}}
   };
\end{tikzpicture}
\end{subfigure}
\caption{Exemple de suppression de la racine : noeud 15}
\end{figure}
\section{Complexité des méthodes principales}
Dans toute cette section, on notera h la hauteur d'un arbre binaire et n son nombre de noeuds.
\subsection{Complexité théorique de la fonction de recherche:}
Le pire cas pour la fonction de recherche est le cas où l'on doit parcourir l'arbre jusqu'à arriver à une feuille. \\
Ainsi, la complexité de la fonction de recherche est en O(h) dans le pire cas. 

Le cas moyen correspond au cas où l'arbre binaire de recherche est de hauteur logarithmique par rapport à n. En effet, un théorème indique que la hauteur d'un arbre généré aléatoirement sera en moyenne logarithmique par rapport à n. \cite{hauteur} \\
Ainsi, la complexité de la fonction de recherche est en O($log_2(n)$) dans le cas moyen.

\subsection{Protocole expérimental:}

\subsubsection{Pire cas:}
Nous avons modélisé le pire cas par des arbres de hauteur égale à leur nombre de sommets.

Graphiquement, cela correspond à des arbres qui ont uniquement des fils gauche ou uniquement des fils droit. Voici un exemple de tel arbre de taille 3:

\begin{center}
\begin{tikzpicture}[very thick,level/.style={sibling distance=30mm/#1}]
\node [vertex] {$1$}
   child{edge from parent[draw=none]}
   child {
    node [vertex]  {$2$}
    child{edge from parent[draw=none]}
    child{node [vertex] {$3$}}
   };
\end{tikzpicture}
\end{center}

Dans ce cas, nous avons h=n, ainsi, la complexité des fonctions principales devrait être linéaire en n. \newline \newline
Nous avons utilisé l'implémentation des arbres binaires de recherche en python.
Nous avons créé de tels arbres de taille allant de 1000 à 100 000 noeuds. Il nous était impossible d'aller plus haut sans faire crasher python.

Pour chaque arbre, nous appelons la fonction étudiée sur une feuille de l'arbre (dans ce cas, il s'agit simplement du noeud de plus grande étiquette) et mesurons le temps que cela prend à l'aide du module time.timeit. 

Nous répétons cette opération 100 fois et cela nous permet de faire une moyenne pour chaque arbre. Nous pouvons ensuite tracer le temps d'exécution en fonction du nombre de noeuds.

Voici les résultats obtenus :

\begin{figure}[h]
\includegraphics[scale=0.65]{images/rech.jpg}
\end{figure}
\begin{figure}[h]
\includegraphics[scale=0.65]{images/ins.jpg}
\end{figure}
\includegraphics[scale=0.65]{images/supp.jpg}



\subsubsection{Cas moyen:}
Nous avons modélisé le cas moyen par des arbres parfaits car d'après la partie 1.3, la hauteur d'un arbre parfait est en $\Theta(log_2(n))$ 

Voici les résultats que nous avons obtenus en reprenant exactement le même protocole expérimental mais en prenant des arbres parfaits: 

\includegraphics[scale=0.75]{images/rech2.jpg} \\
\includegraphics[scale=0.75]{images/ajout2.jpg} \\
\includegraphics[scale=0.75]{images/supp2.jpg}

Les courbes obtenues expérimentalement sont bien logarithmiques.
Cependant, nous avons rencontré quelques difficultés dans l'obtention des courbes pour le cas moyen. 

La principale difficulté était de réussir à créer des arbres parfaits. Les arbres que nous avons généré pour le cas moyen sont "quasiment" parfaits dans le sens où leur hauteur est faible même s'ils ne sont pas exactement parfaits.

De plus, la création d'arbres parfaits est coûteuse en temps et en mémoire: Il n'est pas possible d'en créer des trop gros sans provoquer un dépassement de capacité de la mémoire.
Nous pourrons éventuellement tenter de créer des arbres binaires de grande taille en ajoutant des éléments aux arbres binaires de petite taille mais cela nécessiterait que nous prenions des étiquettes réelles et non plus entières. 


\chapter{Implémentation : comparaison entre structure récursive et représentation en liste}
\section{Structure récursive}
Une représentation qui semble la plus naturelle est d'implémenter une structure avec auto-référence. En effet en s'inspirant de la définition inductive on peut voir un arbre binaire comme une structure contenant essentiellement 3 champs : une clef et deux arbres binaires, ses sous-arbres gauche et droit. 

En terme d'implémentation cela dépend du langage utilisé. 
En C cela passera par la création d'un type spécifique arbre avec en variable des pointeurs vers des variables de ce même type, en plus d'une variable clef de type entier par exemple. 
En suivant le paradigme de la programmation objet (typiquement en java ou en Python), on créé une classe arbre ayant deux attributs de type arbre en plus de l'attribut clef (et d'autres attributs si nécessaires). La souplesse de Python sur les types rend la création de la classe particulièrement aisée mais il faudra être vigilant sur l'utilisation. 

On pourra déclarer la classe comme ceci :

\begin{lstlisting}
class ArbreBinaire():
    def __init__(self,clef,gauche = None ,droite = None):
        """ Anything x ArbreBinaire x ArbreBinaire -> ArbreBinaire
        Constructeur d'arbre"""
        self.clef = clef
        self.gauche = gauche
        self.droite = droite
\end{lstlisting}

Exemple d'instanciation :

\begin{lstlisting}
Feuille1 = ArbreBinaire(1)
Feuille2 = ArbreBinaire(2)
Arbre = ArbreBinaire(3,Feuille1,Feuille2)
\end{lstlisting}

L'arbre créé est :
\begin{center}
\begin{tikzpicture}[very thick,level/.style={sibling distance=30mm/#1}]
\node [vertex] (r){$3$}
  child {
    node [vertex] (a) {$1$}
   }
   child {
    node [vertex] (a) {$2$}
   }
   
   ;
\end{tikzpicture}
\end{center}

\section{Liste}

On peut aussi représenter l'arbre sous forme de liste. L'idée est d'avoir la racine au début de la liste, le fils gauche à l'index 1, le fils droite à l'index 2 et ainsi de suite. Pour un nœud à l'indice i, on trouve son fils gauche à l'indice 2i de la liste, et le droit à l'indice 2i+1. 
On peut donc également retrouver le père d'un nœud facilement : pour un nœud d'index i, le père est à l'index $\lfloor \frac{i-1}{2} \rfloor$ (on prend la partie entière dans la division par 2).
On fera attention à ne pas sauter les nœuds vides, il faut bien les représenter dans la liste par un caractère spécial en fonction de ce que l'arbre contient comme type de donnée. Par exemple mettre False pour un arbre contenant des entiers éventuellement nuls n'est pas une bonne solution, le test False == 0 renvoyant true en Python (et beaucoup d'autres langages).
Par exemple l'arbre suivant :

\begin{center}
\begin{tikzpicture}[very thick,level/.style={sibling distance=30mm/#1}]
\node [vertex] {$5$}
  child {
    node [vertex]  {$2$}
    child{edge from parent[draw=none]}
    child{node [vertex] {$3$}}
   }
   child {
    node [vertex] {$10$}
    child{node [vertex] {$9$}}	
    child{node [vertex] {$12$}}
   };
\end{tikzpicture}
\end{center}

sera représenté par la liste :
$[5,2,10,"vide",3,9,12]$

Remarquons qu'idéalement il faut utiliser une structure de base qui combine les avantages de la liste chaînée et des tableaux : nous avons besoins d'accéder à aux éléments à une position précise en temps constant si possible, mais la structure doit être de taille flexible pour l'agrandir ou la réduire. 

\section{Comparaison}
\subsection{Avantages et inconvénients}
Pour la structure récursive :
\begin{itemize}
\item[•]
Lisibilité du code.
\item[•]
Gain mémoire pour les arbres avec des nœuds vides : on n'insère que les nœuds non vide dans l'arbre.
\item[•]
Plus grande souplesse dans le type de donnée représentée dans le nœud.
\end{itemize}
\paragraph*{}
Pour la représentation en liste :
\begin{itemize}
\item[•]
Gain de mémoire pour les arbres pleins ou parfait.
\item[•]
Simplicité de la mise en place, notamment pour les langages implémentant déjà une structure de liste / tableau (en Python on dispose par exemple déjà des primitives append, pop, len etc).
\item[•]
Accès immédiat au parent.
\end{itemize}
Les inconvénients se déduisent des avantages : pour la structure récursive elle est plus lourde à mettre en place, ça peut être lourd en mémoire pour des arbres pleins.
Pour la liste, le code sera peut être moins lisible et surtout on gâche de la mémoire lorsque l'arbre contient des nœuds vide. 

Dans certains cas la liste est particulièrement appropriée par exemple pour la représentation des tas (arbre binaire parfait sur un ensemble totalement ordonnée où les chemins de la racine vers les feuilles sont toujours croissant), utilisés dans le tri par tas où le but est d'ordonner une liste. 

Pour notre implémentation nous avons choisi la représentation en structure récursive en utilisant la programmation orientée objet en Python.

\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}

Les arbres binaires de recherche sont donc une structure utile pour stocker des données que l'ont peut munir d'une relation d'ordre total, comme des entiers, mais on peut aussi penser à des lettres et donc des mots. En effet dans une telle structure les opérations classiques de recherche, d'insertion et de suppression ont une complexité moyenne logarithmique par rapport au nombre d'élements, les mesures expérimentales ayant confirmé l'étude théorique. Nous verrons par la suite que les quadtree conservent ces propriétés. Nous étudierons les différents types de quadtree (quadtree point et quadtree région notamment) et leurs applications pratiques.


\nocite{*}
\bibliographystyle{plain}
\bibliography{biblio}

\end{document}
